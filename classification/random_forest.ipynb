{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a Random Forest classifier\n",
    "\n",
    "For clarity of code, it is implemented in a different file. Overal, this follows the same guideline as hands_on_classif2_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO ADD : R SQUARED METRIC for hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"Machine learning tools\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import pickle\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.plots import plot_specgram, show_confusion_matrix, plot_decision_boundaries\n",
    "from classification.utils.utils import accuracy\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_dir = \"data/feature_matrices/\" # where to save the features matrices\n",
    "model_dir = \"data/models/\" # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset and compute the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "# print(\"\\n\".join(classnames))\n",
    "\n",
    "fvds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "len_fv = len(fvds[\"birds\", 0])                  # number of items in a feature vector\n",
    "n_fv = len(fvds)                                # number of sounds in the dataset\n",
    "n_audiofiles = dataset.naudio                   # number of audio files in each class\n",
    "n_class = dataset.nclass                        # number of classes\n",
    "\n",
    "train_prop = 0.7                                # proportion of files used for training\n",
    "n_learn = round(train_prop) * n_audiofiles      # number of sounds used for training\n",
    "\n",
    "# Data augmentation not yet implemented\n",
    "data_aug_factor = 1                             # should be an integer\n",
    "class_ids_aug = np.repeat(classnames, n_audiofiles*data_aug_factor)\n",
    "\n",
    "# Compute the maxtrixed dataset\n",
    "X = np.zeros((data_aug_factor*n_class*n_audiofiles, len_fv))\n",
    "for s in range(data_aug_factor):\n",
    "    for class_index, classname in enumerate(classnames): # loop over all classes\n",
    "        for index in range(n_audiofiles): # loop over every audio file of the current class\n",
    "            fv = fvds[classname, index]\n",
    "            X[s*n_class*n_audiofiles + class_index*n_audiofiles+index, :] = fv \n",
    "y = class_ids_aug.copy()\n",
    "\n",
    "np.save(fm_dir+\"feature_matrix_2D_randomforest.npy\", X)\n",
    "print('Shape of the feature matrix : {}'.format(X.shape))\n",
    "print('Number of labels : {}'.format(len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestClassifier = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "# Key parameters are the ones in parenthesis. Some others may be useful to change !\n",
    "\n",
    "# Fit the model\n",
    "class_ids_aug = np.repeat(classnames, n_audiofiles*data_aug_factor) \n",
    "y = class_ids_aug.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1-train_prop), stratify=y) # is stratify necessary ? \n",
    "\n",
    "randomForestClassifier.fit(X_train, y_train)\n",
    "prediction_rf = randomForestClassifier.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy(prediction_rf, y_test)\n",
    "precision_rf = precision_score(prediction_rf, y_test, average=None)\n",
    "recall_rf = recall_score(prediction_rf, y_test, average=None)\n",
    "print(\"recall : \", recall_rf)\n",
    "print(\"precision : \", precision_rf)\n",
    "\n",
    "print('Accuracy of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*accuracy_rf))\n",
    "# print('Precision of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*precision_rf))\n",
    "# print('Recall of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*recall_rf))\n",
    "show_confusion_matrix(prediction_rf, y_test, classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Dataset neither normalized nor reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "accuracy_rf = np.zeros((n_splits,))\n",
    "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
    "  (idx_learn, idx_val) = idx\n",
    "  randomForestClassifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "  prediction_rf = randomForestClassifier.predict(X_train[idx_val])\n",
    "  accuracy_rf[k] = accuracy(prediction_rf, y_train[idx_val])\n",
    "\n",
    "print('Mean accuracy of Random Forest with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.mean()))\n",
    "print('Std deviation in accuracy of KNN with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.std()))\n",
    "\n",
    "filename = 'randomforest.pickle'\n",
    "pickle.dump(randomForestClassifier, open(model_dir+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = np.zeros_like(X_train)\n",
    "for i in range(len(X_train)):\n",
    "  if (np.linalg.norm(X_train[i]) != 0):\n",
    "    X_train_normalized[i] = X_train[i] / np.linalg.norm(X_train[i])\n",
    "\n",
    "rf_classifier_normalized = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "rf_classifier_normalized.fit(X_train_normalized, y_train)\n",
    "\n",
    "X_test_normalized = np.zeros_like(X_test)\n",
    "for i in range(len(X_test)):\n",
    "  if (np.linalg.norm(X_train[i]) != 0):\n",
    "    X_test_normalized[i] = X_test[i] / np.linalg.norm(X_test[i])\n",
    "\n",
    "prediction_rf_normalized = rf_classifier_normalized.predict(X_test_normalized)\n",
    "show_confusion_matrix(prediction_rf_normalized, y_test, classnames) # y_test or train???????????\n",
    "accuracy_rf_normalized = accuracy(prediction_rf_normalized, y_test)\n",
    "print('Accuracy of normalized Random Forest: {:.1f}%'.format(100*accuracy_rf_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "With a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=139 # Number of principal components kept\n",
    "pca = PCA(n_components=n,whiten=True)\n",
    "\n",
    "X_train_reduced = pca.fit_transform(X_train_normalized)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "print('Shape of the reduced learning matrix : {}'.format(X_train_reduced.shape))\n",
    "\n",
    "\n",
    "rf_classifier_reduced = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "rf_classifier_reduced.fit(X_train_reduced, y_train)\n",
    "prediction_rf_reduced = rf_classifier_reduced.predict(X_test_reduced)\n",
    "show_confusion_matrix(prediction_rf_reduced, y_test, classnames)\n",
    "accuracy_rf_reduced = accuracy(prediction_rf_reduced, y_test)\n",
    "print('Accuracy of normalized Random Forest with PCA (dimension reduction): {:.1f}%'.format(100*accuracy_rf_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of hyperparameters\n",
    "\n",
    "Here, we test the impact of some of the hyperparameters of the model on the classifier.\n",
    "\n",
    "Apparently, hyperparameters don't impact so much the classifier, but it is still interesting to optimize the parameters.\n",
    "\n",
    "Parameters analyzed are : n_estimator (number of trees in the forest), min_sample_leaf (minimum number of samples required to be at a leaf internal node, if too small we will capture too much noise), max_depth (maximum depth of a tree).\n",
    "\n",
    "When one parameter is tested, other are set to default values provided in the function.\n",
    "\n",
    "Let us keep in mind that we don't analyze the performance of the model in terms of speed, and that might be a problem for real time applications!\n",
    "\n",
    "In the first time, analysis is done without normalization and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation takes some time, hence leave those values to false if not needed.\n",
    "test_n_estimator = False\n",
    "test_min_sample_leaf = False\n",
    "test_max_depth = False\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "if test_n_estimator == True:\n",
    "    n_estimator = np.arange(1, 500, 25)\n",
    "    accuracies = np.zeros((len(n_estimator), n_splits))\n",
    "    print(n_estimator)\n",
    "    \n",
    "    for i in range(len(n_estimator)):\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimator[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(n_estimator, means, '.-b', label='RF')\n",
    "    plt.fill_between(n_estimator,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('n_estimator')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_min_sample_leaf == True:\n",
    "    min_sample_leaf = np.arange(1, 100, 5)\n",
    "    accuracies = np.zeros((len(min_sample_leaf), n_splits))\n",
    "    print(min_sample_leaf)\n",
    "\n",
    "    for i in range(len(min_sample_leaf)):\n",
    "        classifier = RandomForestClassifier(min_samples_leaf=min_sample_leaf[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(min_sample_leaf, means, '.-b', label='RF')\n",
    "    plt.fill_between(min_sample_leaf,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('min_sample_leaf')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_max_depth == True:\n",
    "    max_depths = np.arange(1, 500, 50)\n",
    "    accuracies = np.zeros((len(max_depths), n_splits))\n",
    "    print(max_depths)\n",
    "\n",
    "    for i in range(len(max_depths)):\n",
    "        classifier = RandomForestClassifier(max_depth=max_depths[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(max_depths, means, '.-b', label='RF')\n",
    "    plt.fill_between(max_depths,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioUtilInst = AudioUtil()\n",
    "\n",
    "fvds.mod_data_aug(['add_noise'])\n",
    "data_aug_factor = 2\n",
    "y_aug = np.repeat(classnames, dataset.naudio*fvds.data_aug_factor) # Labels\n",
    "\n",
    "\n",
    "X_aug = np.zeros((fvds.data_aug_factor*n_class*n_audiofiles, len_fv))\n",
    "for s in range(fvds.data_aug_factor):\n",
    "    for index in range(dataset.naudio):\n",
    "        for class_index, classname in enumerate(classnames):\n",
    "            fv = fvds[classname, index]\n",
    "            X_aug[s*n_class*n_audiofiles + class_index*n_audiofiles + index, :] = fv\n",
    "\n",
    "# np.save(fm_dir+\"feature_matrix_2D_aug_RF\")\n",
    "\n",
    "# Manually modify the 200 last elems to add noise. This is a very bad implementation, it should change\n",
    "for i in range(200, 400):\n",
    "    fv = X_aug[i].copy()\n",
    "    fv_noise, sr = AudioUtilInst.add_noise((fv,1)) # Arbitrary sample rate, we don't care about it\n",
    "    X_aug[i] = fv_noise\n",
    "\n",
    "# Instantiate a new classifier\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "\n",
    "# Fit the model\n",
    "class_ids_aug = np.repeat(classnames, n_audiofiles*data_aug_factor) \n",
    "y = class_ids_aug.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_aug, y_aug, test_size=(1-train_prop), stratify=y) # is stratify necessary ? \n",
    "\n",
    "randomForestClassifier.fit(X_train, y_train)\n",
    "prediction_rf = randomForestClassifier.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy(prediction_rf, y_test)\n",
    "precision_rf = precision_score(y_test, prediction_rf, average=None)\n",
    "recall_rf = recall_score(y_test, prediction_rf, average=None)\n",
    "print(\"recall : \", recall_rf)\n",
    "print(\"precision : \", precision_rf)\n",
    "\n",
    "print('Accuracy of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*accuracy_rf))\n",
    "show_confusion_matrix(prediction_rf, y_test, classnames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation with augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "accuracy_rf = np.zeros((n_splits,))\n",
    "precision_rf = np.zeros((5,))\n",
    "recall_rf = np.zeros((5,))\n",
    "\n",
    "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
    "  (idx_learn, idx_val) = idx\n",
    "  randomForestClassifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "  prediction_rf = randomForestClassifier.predict(X_train[idx_val])\n",
    "  accuracy_rf[k] = accuracy(prediction_rf, y_train[idx_val])\n",
    "  precision_rf += precision_score(y_train[idx_val], prediction_rf, average=None)\n",
    "  recall_rf += recall_score(y_train[idx_val], prediction_rf, average=None)\n",
    "\n",
    "recall_rf /= n_splits\n",
    "precision_rf /= n_splits\n",
    "\n",
    "print(\"Mean recall : \", recall_rf)\n",
    "print(\"Mean precision : \", precision_rf)\n",
    "\n",
    "print('Mean accuracy of Random Forest with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.mean()))\n",
    "print('Std deviation in accuracy of KNN with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk\n",
    "# Simulation takes some time, hence leave those values to false if not needed.\n",
    "test_n_estimator = True\n",
    "test_min_sample_leaf = False\n",
    "test_max_depth = False\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "if test_n_estimator == True:\n",
    "    n_estimator = np.arange(1, 150, 3)\n",
    "    accuracies = np.zeros((len(n_estimator), n_splits))\n",
    "    print(n_estimator)\n",
    "    \n",
    "    for i in range(len(n_estimator)):\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimator[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(n_estimator, means, '.-b', label='RF')\n",
    "    plt.fill_between(n_estimator,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('n_estimator')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_min_sample_leaf == True:\n",
    "    min_sample_leaf = np.arange(1, 100, 5)\n",
    "    accuracies = np.zeros((len(min_sample_leaf), n_splits))\n",
    "    print(min_sample_leaf)\n",
    "\n",
    "    for i in range(len(min_sample_leaf)):\n",
    "        classifier = RandomForestClassifier(min_samples_leaf=min_sample_leaf[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(min_sample_leaf, means, '.-b', label='RF')\n",
    "    plt.fill_between(min_sample_leaf,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('min_sample_leaf')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_max_depth == True:\n",
    "    max_depths = np.arange(1, 500, 50)\n",
    "    accuracies = np.zeros((len(max_depths), n_splits))\n",
    "    print(max_depths)\n",
    "\n",
    "    for i in range(len(max_depths)):\n",
    "        classifier = RandomForestClassifier(max_depth=max_depths[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(max_depths, means, '.-b', label='RF')\n",
    "    plt.fill_between(max_depths,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lelec210x-1z0NCqGZ-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
