{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a Random Forest classifier\n",
    "\n",
    "For clarity of code, it is implemented in a different file. Overal, this follows the same guideline as hands_on_classif2_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO ADD : R SQUARED METRIC for hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"Machine learning tools\"\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score, precision_score\n",
    "import pickle\n",
    "\n",
    "from classification.datasets import Dataset\n",
    "from classification.utils.plots import plot_specgram, show_confusion_matrix, plot_decision_boundaries\n",
    "from classification.utils.utils import accuracy\n",
    "from classification.utils.audio_student import AudioUtil, Feature_vector_DS\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_dir = \"data/feature_matrices/\" # where to save the features matrices\n",
    "model_dir = \"data/models/\" # where to save the models\n",
    "os.makedirs(fm_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset and compute the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature matrix : (200, 400)\n",
      "Number of labels : 200\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = Dataset()\n",
    "classnames = dataset.list_classes()\n",
    "\n",
    "# print(\"\\n\".join(classnames))\n",
    "\n",
    "fvds = Feature_vector_DS(dataset, Nft=512, nmel=20, duration=950, shift_pct=0.0)\n",
    "\n",
    "len_fv = len(fvds[\"birds\", 0])                  # number of items in a feature vector\n",
    "n_fv = len(fvds)                                # number of sounds in the dataset\n",
    "n_audiofiles = dataset.naudio                   # number of audio files in each class\n",
    "n_class = dataset.nclass                        # number of classes\n",
    "\n",
    "train_prop = 0.7                                # proportion of files used for training\n",
    "n_learn = round(train_prop) * n_audiofiles      # number of sounds used for training\n",
    "\n",
    "# Data augmentation not yet implemented\n",
    "data_aug_factor = 1                             # should be an integer\n",
    "class_ids_aug = np.repeat(classnames, n_audiofiles*data_aug_factor)\n",
    "\n",
    "# Compute the maxtrixed dataset\n",
    "X = np.zeros((data_aug_factor*n_class*n_audiofiles, len_fv))\n",
    "for s in range(data_aug_factor):\n",
    "    for class_index, classname in enumerate(classnames): # loop over all classes\n",
    "        for index in range(n_audiofiles): # loop over every audio file of the current class\n",
    "            fv = fvds[classname, index]\n",
    "            X[s*n_class*n_audiofiles + class_index*n_audiofiles+index, :] = fv \n",
    "y = class_ids_aug.copy()\n",
    "\n",
    "np.save(fm_dir+\"feature_matrix_2D_randomforest.npy\", X)\n",
    "print('Shape of the feature matrix : {}'.format(X.shape))\n",
    "print('Number of labels : {}'.format(len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall : birds : 0.800 | chainsaw : 0.750 | fire : 0.632 | handsaw : 0.700 | helicopter : 0.800\n",
      "Precision : birds : 0.667 | chainsaw : 0.692 | fire : 1.000 | handsaw : 0.583 | helicopter : 0.667\n",
      "Accuracy of Random Forest with fixed train/validation sets : 72.1%\n",
      "0.7213114754098361\n",
      "[0.66666667 0.69230769 1.         0.58333333 0.66666667]\n"
     ]
    }
   ],
   "source": [
    "randomForestClassifier = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "# Key parameters are the ones in parenthesis. Some others may be useful to change !\n",
    "\n",
    "# Fit the model\n",
    "class_ids_aug = np.repeat(classnames, n_audiofiles*data_aug_factor) \n",
    "y = class_ids_aug.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=(1-train_prop), stratify=y) # is stratify necessary ? \n",
    "\n",
    "randomForestClassifier.fit(X_train, y_train)\n",
    "prediction_rf = randomForestClassifier.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy(prediction_rf, y_test)\n",
    "precision_rf = precision_score(prediction_rf, y_test, average=None)\n",
    "recall_rf = recall_score(prediction_rf, y_test, average=None)\n",
    "print(\"Recall : {} : {:.3f} | {} : {:.3f} | {} : {:.3f} | {} : {:.3f} | {} : {:.3f}\".format(classnames[0],recall_rf[0],\n",
    "                                                                                            classnames[1],recall_rf[1],\n",
    "                                                                                            classnames[2],recall_rf[2],\n",
    "                                                                                            classnames[3],recall_rf[3],\n",
    "                                                                                            classnames[4],recall_rf[4]))\n",
    "\n",
    "print(\"Precision : {} : {:.3f} | {} : {:.3f} | {} : {:.3f} | {} : {:.3f} | {} : {:.3f}\".format( classnames[0],precision_rf[0],\n",
    "                                                                                                classnames[1],precision_rf[1],\n",
    "                                                                                                classnames[2],precision_rf[2],\n",
    "                                                                                                classnames[3],precision_rf[3],\n",
    "                                                                                                classnames[4],precision_rf[4]))\n",
    "\n",
    "print('Accuracy of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*accuracy_rf))\n",
    "# print('Precision of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*precision_rf))\n",
    "# print('Recall of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*recall_rf))\n",
    "show_confusion_matrix(prediction_rf, y_test, classnames)\n",
    "\n",
    "print(accuracy_rf)\n",
    "print(precision_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "Dataset neither normalized nor reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of Random Forest with n_splits-Fold CV: 60.4%\n",
      "Std deviation in accuracy of KNN with n_splits-Fold CV: 8.2%\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "accuracy_rf = np.zeros((n_splits,))\n",
    "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
    "  (idx_learn, idx_val) = idx\n",
    "  randomForestClassifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "  prediction_rf = randomForestClassifier.predict(X_train[idx_val])\n",
    "  accuracy_rf[k] = accuracy(prediction_rf, y_train[idx_val])\n",
    "\n",
    "print('Mean accuracy of Random Forest with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.mean()))\n",
    "print('Std deviation in accuracy of KNN with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.std()))\n",
    "\n",
    "filename = 'randomforest.pickle'\n",
    "pickle.dump(randomForestClassifier, open(model_dir+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of Random Forest with n_splits-Fold CV: 64.5%\n",
      "Std deviation in accuracy of Random Forest with n_splits-Fold CV: 5.4%\n",
      "Mean precision of Random Forest with n_splits-Fold CV:  [57.5 60.  72.5 65.  67.5]\n",
      "Mean recall of Random Forest with n_splits-Fold CV:  [70.53571429 63.88888889 61.56517094 67.82106782 63.83699634]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "# X_train_normalized = np.zeros_like(X_train)\n",
    "# for i in range(len(X_train)):\n",
    "#   if (np.linalg.norm(X_train[i]) != 0):\n",
    "#     X_train_normalized[i] = X_train[i] / np.linalg.norm(X_train[i])\n",
    "\n",
    "# rf_classifier_normalized = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "# rf_classifier_normalized.fit(X_train_normalized, y_train)\n",
    "\n",
    "# X_test_normalized = np.zeros_like(X_test)\n",
    "# for i in range(len(X_test)):\n",
    "#   if (np.linalg.norm(X_train[i]) != 0):\n",
    "#     X_test_normalized[i] = X_test[i] / np.linalg.norm(X_test[i])\n",
    "\n",
    "# prediction_rf_normalized = rf_classifier_normalized.predict(X_test_normalized)\n",
    "# show_confusion_matrix(prediction_rf_normalized, y_test, classnames) # y_test or train???????????\n",
    "# accuracy_rf_normalized = accuracy(prediction_rf_normalized, y_test)\n",
    "# print('Accuracy of normalized Random Forest: {:.1f}%'.format(100*accuracy_rf_normalized))\n",
    "\n",
    "\n",
    "X_train_normalized = np.zeros_like(X_train)\n",
    "for i in range(len(X_train)):\n",
    "  if (np.linalg.norm(X_train[i]) != 0):\n",
    "    mean = np.mean(X_train[i])\n",
    "    X_train_normalized[i] = X_train[i] / mean\n",
    "\n",
    "X_test_normalized = np.zeros_like(X_test)\n",
    "for i in range(len(X_test)):\n",
    "  if (np.linalg.norm(X_test[i]) != 0):\n",
    "    mean = np.mean(X_test[i])\n",
    "    X_test_normalized[i] = X_test[i] / mean\n",
    "\n",
    "rf_classifier_normalized = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "rf_classifier_normalized.fit(X_train_normalized, y_train)\n",
    "\n",
    "# prediction_rf_normalized = rf_classifier_normalized.predict(X_test_normalized)\n",
    "# show_confusion_matrix(prediction_rf_normalized, y_test, classnames) # y_test or train???????????\n",
    "# accuracy_rf_normalized = accuracy(prediction_rf_normalized, y_test)\n",
    "# print('Accuracy of normalized Random Forest: {:.1f}%'.format(100*accuracy_rf_normalized))\n",
    "\n",
    "#%matplotlib inline \n",
    "#%matplotlib qt\n",
    "X_normalized = np.zeros_like(X)\n",
    "# for i in range(len(X)):\n",
    "#   if (np.linalg.norm(X[i]) != 0):\n",
    "#     mean = np.mean(X[i])\n",
    "#     X_normalized[i] = X[i] / mean\n",
    "\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20, min_samples_leaf=3)\n",
    "\n",
    "for i in range(len(X)):\n",
    "  if (np.linalg.norm(X[i]) != 0):\n",
    "    mean = np.mean(X[i])\n",
    "    l = len(X[i])\n",
    "    for j in range(l):\n",
    "      if (X[i][j] > 0.2*mean): # threshold : 0.2 * mean\n",
    "        X_normalized[i][j] = X[i][j] / mean\n",
    "\n",
    "n_splits = 4\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "# To plot the global confusion matrix\n",
    "all_prediction = []\n",
    "all_true = [] \n",
    " \n",
    "\n",
    "display_conf_matrix = True\n",
    "accuracy_rf = np.zeros((n_splits,))\n",
    "precision_rf = np.zeros((n_splits, 5))\n",
    "recall_rf = np.zeros((n_splits, 5))\n",
    "\n",
    "for k, idx in enumerate(kf.split(X, y)):\n",
    "  (idx_learn, idx_val) = idx\n",
    "  randomForestClassifier.fit(X_normalized[idx_learn], y[idx_learn])\n",
    "  prediction_rf = randomForestClassifier.predict(X_normalized[idx_val])\n",
    "  accuracy_rf[k] = accuracy(prediction_rf, y[idx_val])\n",
    "  precision_rf[k] = precision_score(y[idx_val], prediction_rf, average=None)\n",
    "  recall_rf[k] = recall_score(y[idx_val], prediction_rf, average=None)\n",
    "\n",
    "  all_prediction.extend(prediction_rf)\n",
    "  all_true.extend(y[idx_val])\n",
    "\n",
    "  if (display_conf_matrix and k==n_splits - 1):\n",
    "    show_confusion_matrix(all_prediction , all_true ,classnames)\n",
    "    #display_conf_matrix = False\n",
    "\n",
    "\n",
    "print('Mean accuracy of Random Forest with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.mean()))\n",
    "print('Std deviation in accuracy of Random Forest with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.std()))\n",
    "\n",
    "print('Mean precision of Random Forest with n_splits-Fold CV: ', (100*np.mean(precision_rf, axis=0)))\n",
    "print('Mean recall of Random Forest with n_splits-Fold CV: ', (100*np.mean(recall_rf, axis=0)))\n",
    "\n",
    "filename = 'randomforestNormalized.pickle'\n",
    "pickle.dump(randomForestClassifier, open(model_dir+filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 400)\n",
      "0.665\n",
      "[0.86666667 0.46341463 0.68181818 0.87804878 1.        ]\n",
      "[0.65  0.95  0.75  0.9   0.075]\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for normalized classifier\n",
    "\n",
    "# Modifies the normal dataset to induce noise and echo\n",
    "AudioUtilInst = AudioUtil()\n",
    "\n",
    "X_corr = X.copy() # X corrupted\n",
    "y_corr = y.copy() # Nothing to change\n",
    "\n",
    "print(X_corr.shape)\n",
    "\n",
    "# X_corr = AudioUtilInst.add_bg((X_corr, -1), dataset) # add background noises\n",
    "# X_corr = AudioUtilInst.echo((X_corr, -1), nechos=2) # adds echo\n",
    "# X_corr = AudioUtilInst.add_noise((X_corr, -1)) # adds noise\n",
    "\n",
    "for i in range(len(X_corr)):\n",
    "  # X_corr[i] = AudioUtilInst.add_bg((X_corr[i], -1), dataset)[0]\n",
    "  X_corr[i] = AudioUtilInst.echo((X_corr[i], -1), nechos=2)[0]\n",
    "  # X_corr[i] = AudioUtilInst.add_noise((X_corr[i], -1), 0.1)[0]\n",
    "  \n",
    "# Normalizes dataset\n",
    "for i in range(len(X_corr)):\n",
    "  if (np.linalg.norm(X_corr[i]) != 0):\n",
    "    mean = np.mean(X_corr[i])\n",
    "    l = len(X[i])\n",
    "    for j in range(l):\n",
    "      if (X_corr[i][j] > 0.2*mean): # threshold : 0.2 * mean\n",
    "        X_corr[i][j] = X_corr[i][j] / mean\n",
    "\n",
    "prediction_corr = randomForestClassifier.predict(X_corr)\n",
    "accuracy_corr = accuracy(prediction_corr, y)\n",
    "precision_corr = precision_score(y, prediction_corr, average=None)\n",
    "recall_corr = recall_score(y, prediction_corr, average=None)\n",
    "\n",
    "print(accuracy_corr)\n",
    "print(precision_corr)\n",
    "print(recall_corr)\n",
    "\n",
    "show_confusion_matrix(prediction_corr, y, classnames)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(all_prediction , all_true ,classnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimension reduction\n",
    "\n",
    "With a PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the reduced learning matrix : (139, 139)\n",
      "Accuracy of normalized Random Forest with PCA (dimension reduction): 23.0%\n"
     ]
    }
   ],
   "source": [
    "n=139 # Number of principal components kept\n",
    "pca = PCA(n_components=n,whiten=True)\n",
    "\n",
    "X_train_reduced = pca.fit_transform(X_train_normalized)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "print('Shape of the reduced learning matrix : {}'.format(X_train_reduced.shape))\n",
    "\n",
    "\n",
    "rf_classifier_reduced = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "rf_classifier_reduced.fit(X_train_reduced, y_train)\n",
    "prediction_rf_reduced = rf_classifier_reduced.predict(X_test_reduced)\n",
    "show_confusion_matrix(prediction_rf_reduced, y_test, classnames)\n",
    "accuracy_rf_reduced = accuracy(prediction_rf_reduced, y_test)\n",
    "print('Accuracy of normalized Random Forest with PCA (dimension reduction): {:.1f}%'.format(100*accuracy_rf_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of hyperparameters\n",
    "\n",
    "Here, we test the impact of some of the hyperparameters of the model on the classifier.\n",
    "\n",
    "Apparently, hyperparameters don't impact so much the classifier, but it is still interesting to optimize the parameters.\n",
    "\n",
    "Parameters analyzed are : n_estimator (number of trees in the forest), min_sample_leaf (minimum number of samples required to be at a leaf internal node, if too small we will capture too much noise), max_depth (maximum depth of a tree).\n",
    "\n",
    "When one parameter is tested, other are set to default values provided in the function.\n",
    "\n",
    "Let us keep in mind that we don't analyze the performance of the model in terms of speed, and that might be a problem for real time applications!\n",
    "\n",
    "In the first time, analysis is done without normalization and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation takes some time, hence leave those values to false if not needed.\n",
    "test_n_estimator = False\n",
    "test_min_sample_leaf = False\n",
    "test_max_depth = False\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "if test_n_estimator == True:\n",
    "    n_estimator = np.arange(1, 500, 25)\n",
    "    accuracies = np.zeros((len(n_estimator), n_splits))\n",
    "    print(n_estimator)\n",
    "    \n",
    "    for i in range(len(n_estimator)):\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimator[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(n_estimator, means, '.-b', label='RF')\n",
    "    plt.fill_between(n_estimator,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('n_estimator')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_min_sample_leaf == True:\n",
    "    min_sample_leaf = np.arange(1, 100, 5)\n",
    "    accuracies = np.zeros((len(min_sample_leaf), n_splits))\n",
    "    print(min_sample_leaf)\n",
    "\n",
    "    for i in range(len(min_sample_leaf)):\n",
    "        classifier = RandomForestClassifier(min_samples_leaf=min_sample_leaf[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(min_sample_leaf, means, '.-b', label='RF')\n",
    "    plt.fill_between(min_sample_leaf,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('min_sample_leaf')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_max_depth == True:\n",
    "    max_depths = np.arange(1, 500, 50)\n",
    "    accuracies = np.zeros((len(max_depths), n_splits))\n",
    "    print(max_depths)\n",
    "\n",
    "    for i in range(len(max_depths)):\n",
    "        classifier = RandomForestClassifier(max_depth=max_depths[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(max_depths, means, '.-b', label='RF')\n",
    "    plt.fill_between(max_depths,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall :  [0.79166667 0.625      0.375      0.58333333 0.52      ]\n",
      "precision :  [0.57575758 0.75       0.64285714 0.5        0.5       ]\n",
      "Accuracy of Random Forest with fixed train/validation sets : 57.9%\n"
     ]
    }
   ],
   "source": [
    "AudioUtilInst = AudioUtil()\n",
    "\n",
    "fvds.mod_data_aug(['add_noise'])\n",
    "data_aug_factor = 2\n",
    "y_aug = np.repeat(classnames, dataset.naudio*fvds.data_aug_factor) # Labels\n",
    "\n",
    "\n",
    "X_aug = np.zeros((fvds.data_aug_factor*n_class*n_audiofiles, len_fv))\n",
    "for s in range(fvds.data_aug_factor):\n",
    "    for index in range(dataset.naudio):\n",
    "        for class_index, classname in enumerate(classnames):\n",
    "            fv = fvds[classname, index]\n",
    "            X_aug[s*n_class*n_audiofiles + class_index*n_audiofiles + index, :] = fv\n",
    "\n",
    "# np.save(fm_dir+\"feature_matrix_2D_aug_RF\")\n",
    "\n",
    "# Manually modify the 200 last elems to add noise. This is a very bad implementation, it should change\n",
    "for i in range(200, 400):\n",
    "    fv = X_aug[i].copy()\n",
    "    fv_noise, sr = AudioUtilInst.add_noise((fv,1)) # Arbitrary sample rate, we don't care about it\n",
    "    X_aug[i] = fv_noise\n",
    "\n",
    "# Instantiate a new classifier\n",
    "randomForestClassifier = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=20)\n",
    "\n",
    "# Fit the model\n",
    "class_ids_aug = np.repeat(classnames, n_audiofiles*data_aug_factor) \n",
    "y = class_ids_aug.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_aug, y_aug, test_size=(1-train_prop), stratify=y) # is stratify necessary ? \n",
    "\n",
    "randomForestClassifier.fit(X_train, y_train)\n",
    "prediction_rf = randomForestClassifier.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy(prediction_rf, y_test)\n",
    "precision_rf = precision_score(y_test, prediction_rf, average=None)\n",
    "recall_rf = recall_score(y_test, prediction_rf, average=None)\n",
    "print(\"recall : \", recall_rf)\n",
    "print(\"precision : \", precision_rf)\n",
    "\n",
    "print('Accuracy of Random Forest with fixed train/validation sets : {:.1f}%'.format(100*accuracy_rf))\n",
    "show_confusion_matrix(prediction_rf, y_test, classnames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation with augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\renar\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\lelec210x-1z0NCqGZ-py3.9\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean recall :  [0.52333333 0.62333333 0.33333333 0.53666667 0.34666667]\n",
      "Mean precision :  [0.39361111 0.59833333 0.38857143 0.56884921 0.45404762]\n",
      "Mean accuracy of Random Forest with n_splits-Fold CV: 47.0%\n",
      "Std deviation in accuracy of KNN with n_splits-Fold CV: 6.5%\n"
     ]
    }
   ],
   "source": [
    "n_splits = 10\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "accuracy_rf = np.zeros((n_splits,))\n",
    "precision_rf = np.zeros((5,))\n",
    "recall_rf = np.zeros((5,))\n",
    "\n",
    "for k, idx in enumerate(kf.split(X_train,y_train)):\n",
    "  (idx_learn, idx_val) = idx\n",
    "  randomForestClassifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "  prediction_rf = randomForestClassifier.predict(X_train[idx_val])\n",
    "  accuracy_rf[k] = accuracy(prediction_rf, y_train[idx_val])\n",
    "  precision_rf += precision_score(y_train[idx_val], prediction_rf, average=None)\n",
    "  recall_rf += recall_score(y_train[idx_val], prediction_rf, average=None)\n",
    "\n",
    "recall_rf /= n_splits\n",
    "precision_rf /= n_splits\n",
    "\n",
    "print(\"Mean recall : \", recall_rf)\n",
    "print(\"Mean precision : \", precision_rf)\n",
    "\n",
    "print('Mean accuracy of Random Forest with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.mean()))\n",
    "print('Std deviation in accuracy of KNN with n_splits-Fold CV: {:.1f}%'.format(100*accuracy_rf.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib tk\n",
    "# Simulation takes some time, hence leave those values to false if not needed.\n",
    "test_n_estimator = False\n",
    "test_min_sample_leaf = False\n",
    "test_max_depth = False\n",
    "\n",
    "n_splits = 5\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "if test_n_estimator == True:\n",
    "    n_estimator = np.arange(1, 150, 3)\n",
    "    accuracies = np.zeros((len(n_estimator), n_splits))\n",
    "    print(n_estimator)\n",
    "    \n",
    "    for i in range(len(n_estimator)):\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimator[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(n_estimator, means, '.-b', label='RF')\n",
    "    plt.fill_between(n_estimator,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('n_estimator')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_min_sample_leaf == True:\n",
    "    min_sample_leaf = np.arange(1, 100, 5)\n",
    "    accuracies = np.zeros((len(min_sample_leaf), n_splits))\n",
    "    print(min_sample_leaf)\n",
    "\n",
    "    for i in range(len(min_sample_leaf)):\n",
    "        classifier = RandomForestClassifier(min_samples_leaf=min_sample_leaf[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(min_sample_leaf, means, '.-b', label='RF')\n",
    "    plt.fill_between(min_sample_leaf,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('min_sample_leaf')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_max_depth == True:\n",
    "    max_depths = np.arange(1, 500, 50)\n",
    "    accuracies = np.zeros((len(max_depths), n_splits))\n",
    "    print(max_depths)\n",
    "\n",
    "    for i in range(len(max_depths)):\n",
    "        classifier = RandomForestClassifier(max_depth=max_depths[i])\n",
    "        for k, index in enumerate(kf.split(X_train,y_train)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_train[idx_learn], y_train[idx_learn])\n",
    "                prediction = classifier.predict(X_train[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y_train[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(max_depths, means, '.-b', label='RF')\n",
    "    plt.fill_between(max_depths,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters for the normalized model\n",
    "\n",
    "test_n_estimator = False\n",
    "test_min_sample_leaf = False\n",
    "test_max_depth = False\n",
    "\n",
    "print(len(X_normalized))\n",
    "\n",
    "n_splits = 4\n",
    "kf = StratifiedKFold(n_splits=n_splits,shuffle=True)\n",
    "\n",
    "if test_n_estimator == True:\n",
    "    n_estimator = np.arange(1, 150, 3)\n",
    "    accuracies = np.zeros((len(n_estimator), n_splits))\n",
    "    print(n_estimator)\n",
    "    \n",
    "    for i in range(len(n_estimator)):\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimator[i], max_depth=20)\n",
    "        for k, index in enumerate(kf.split(X_normalized,y)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_normalized[idx_learn], y[idx_learn])\n",
    "                prediction = classifier.predict(X_normalized[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(n_estimator, means, '.-b', label='RF')\n",
    "    plt.fill_between(n_estimator,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('n_estimator')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_min_sample_leaf == True:\n",
    "    min_sample_leaf = np.arange(1, 100, 5)\n",
    "    accuracies = np.zeros((len(min_sample_leaf), n_splits))\n",
    "    print(min_sample_leaf)\n",
    "\n",
    "    for i in range(len(min_sample_leaf)):\n",
    "        classifier = RandomForestClassifier(min_samples_leaf=min_sample_leaf[i])\n",
    "        for k, index in enumerate(kf.split(X_normalized,y)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_normalized[idx_learn], y[idx_learn])\n",
    "                prediction = classifier.predict(X_normalized[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(min_sample_leaf, means, '.-b', label='RF')\n",
    "    plt.fill_between(min_sample_leaf,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('min_sample_leaf')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if test_max_depth == True:\n",
    "    max_depths = np.arange(1, 500, 50)\n",
    "    accuracies = np.zeros((len(max_depths), n_splits))\n",
    "    print(max_depths)\n",
    "\n",
    "    for i in range(len(max_depths)):\n",
    "        classifier = RandomForestClassifier(max_depth=max_depths[i])\n",
    "        for k, index in enumerate(kf.split(X_normalized, y)):\n",
    "                (idx_learn, idx_val) = index\n",
    "                classifier.fit(X_normalized[idx_learn], y[idx_learn])\n",
    "                prediction = classifier.predict(X_normalized[idx_val])\n",
    "                accuracies[i,k] = accuracy(prediction, y[idx_val])\n",
    "    means = accuracies.mean(axis=1) \n",
    "    stds = accuracies.std(axis=1) \n",
    "\n",
    "    \"Plot\"\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.plot(max_depths, means, '.-b', label='RF')\n",
    "    plt.fill_between(max_depths,means-stds,means+stds,alpha=0.2,color='b')\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlabel('max_depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Master1\\Q1\\LELEC210x\\LELEC210X\\classification\\src\\classification\\utils\\audio_student.py:408: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(10, 5)) # modified from (4,3)\n"
     ]
    }
   ],
   "source": [
    "# Print melspecs for different sounds\n",
    "\n",
    "for i in range(39):\n",
    "    sound = fvds.display(('birds', i)) #11, 15, 22\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lelec210x-1z0NCqGZ-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
